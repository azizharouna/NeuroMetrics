{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelization \n",
    "For this analysis, let's consider a range of regression models:\n",
    "\n",
    "* Linear Regression: A simple baseline model.\n",
    "* Ridge Regression: Linear regression with L2 regularization.\n",
    "* Lasso Regression: Linear regression with L1 regularization.\n",
    "* Random Forest Regressor: A decision tree-based ensemble method.\n",
    "* Gradient Boosting Regressor: Boosting-based ensemble method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Getting the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utilspro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"3_data_preprocessing.ipynb\"\n",
    "#execute_notebook(\"3_data_preprocessing.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Linear Regression': 88.27013283650295,\n",
       " 'Ridge Regression': 88.02320099992671,\n",
       " 'Lasso Regression': 87.36392537963899,\n",
       " 'Random Forest Regressor': 87.89870580270997,\n",
       " 'Gradient Boosting Regressor': 78.91591652938544}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'Random Forest Regressor': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Create pipelines for each model\n",
    "pipelines = {name: Pipeline([('model', model)]) for name, model in models.items()}\n",
    "\n",
    "# Train and evaluate each pipeline\n",
    "results = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    predictions = pipeline.predict(X_test_scaled)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    results[name] = mse\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting Regressor has the lowest MSE, making it the best-performing model among the ones we evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's focus on hyperparameter tuning for the best-performing model, which is the Gradient Boosting Regressor\n",
    "\n",
    "* n_estimators: The number of boosting stages to be run.\n",
    "* learning_rate: Determines the contribution of each tree to the final prediction.\n",
    "* max_depth: The maximum depth of the individual regression estimators.\n",
    "* min_samples_split: The minimum number of samples required to split an internal node.\n",
    "* min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "We will define a grid for these hyperparameters and perform a grid search to find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1,\n",
       "  'max_depth': 4,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_samples_split': 2,\n",
       "  'n_estimators': 50},\n",
       " 77.36852687070196)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the hyperparameter grid for Gradient Boosting Regressor\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object for Gradient Boosting Regressor\n",
    "grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and the corresponding score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_\n",
    "\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1,\n",
       "  'max_depth': 3,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_samples_split': 4,\n",
       "  'n_estimators': 100},\n",
       " 77.84541133733364)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a reduced hyperparameter grid for Gradient Boosting Regressor\n",
    "reduced_param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 4],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object for Gradient Boosting Regressor with reduced grid\n",
    "reduced_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), reduced_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model with reduced grid\n",
    "reduced_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and the corresponding score from the reduced grid search\n",
    "reduced_best_params = reduced_grid_search.best_params_\n",
    "reduced_best_score = -reduced_grid_search.best_score_\n",
    "\n",
    "reduced_best_params, reduced_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 150,\n",
       "  'min_samples_split': 4,\n",
       "  'min_samples_leaf': 1,\n",
       "  'max_depth': 3,\n",
       "  'learning_rate': 0.1},\n",
       " 79.00452234510185)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RandomizedSearchCV object for Gradient Boosting Regressor\n",
    "random_search = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,  # Number of parameter settings that are sampled\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model with random search\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and the corresponding score from the randomized search\n",
    "random_best_params = random_search.best_params_\n",
    "random_best_score = -random_search.best_score_\n",
    "\n",
    "random_best_params, random_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.5, 1],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 0.01, 0.05, 0.1, 0.2]\n",
    "}\n",
    "# Create a GridSearchCV object for Gradient Boosting Regressor with refined grid\n",
    "refined_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), refined_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model with reduced grid\n",
    "refined_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and the corresponding score from the reduced refined grid search\n",
    "\n",
    "refined_best_params = refined_grid_search.best_params_\n",
    "refined_best_score = refined_grid_search.best_score_\n",
    "\n",
    "refined_best_params, refined_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model to the local file system\n",
    "filename = 'data/finalized_model.sav'\n",
    "pickle.dump(random_search, open(filename, 'wb'))    \n",
    "#saving the scaler\n",
    "filename = 'data/finalized_scaler.sav'\n",
    "pickle.dump(scaler, open(filename, 'wb'))\n",
    "#saving the encoder\n",
    "filename = 'data/finalized_encoder.sav'\n",
    "pickle.dump(encoder, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
